{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 2. Clustering using k-means\n",
    "\n",
    "### Student ID: 915030521\n",
    "### Student Name: Chenghan Sun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named '_geoslib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-7001c6e51bf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmpl_toolkits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasemap\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBasemap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpylab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrcParams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/mpl_toolkits/basemap/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0m_geoslib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named '_geoslib'"
     ]
    }
   ],
   "source": [
    "# All Import Statements Defined Here\n",
    "# Note: Do not change anything\n",
    "\n",
    "import numpy\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Do not use any other packages below here in your code before part 4\n",
    "# install Basemap before you start\n",
    "\n",
    "import pandas as pd\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from pylab import rcParams\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Implementing k-means algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete what is missing to implement the k means algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class k_means:\n",
    "    \n",
    "    def __init__(self, data: numpy.ndarray, d: int, k: int , tol: float, max_iter: int):\n",
    "        \"\"\"\n",
    "        data: data to cluster\n",
    "        d:dimension of the data\n",
    "        k: prespecified number of clusters\n",
    "        tol: convergence criterion\n",
    "        max_iter: maximum number of iterations allowed\n",
    "        \"\"\"\n",
    "        self.partitions={i:[] for i in range(k) }\n",
    "        self.labels=[] # list of numbers with values from 0 to k-1\n",
    "        self.d=d\n",
    "        self.n=data.shape[0]  # num of samples \n",
    "        self.counter=0\n",
    "    \n",
    "        ### your code starts here\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        self.data = data\n",
    "        self.update_flag = True\n",
    "        self.centers = None\n",
    "        self.labels = {i:[] for i in range(self.k)}\n",
    "        self.cost = 0\n",
    "        ### end of your code\n",
    "    \n",
    "    def initialize_centers(self, method: int):\n",
    "        \"\"\"\n",
    "        method = 1:\n",
    "        randomly pick k points from the data as centers\n",
    "        \"\"\"\n",
    "        if method==0:\n",
    "            self.centers=data[:self.k,:]\n",
    "            \n",
    "        elif method==1:\n",
    "        ### your code starts here\n",
    "            rand_index = numpy.random.choice(self.n, self.k, replace=False)\n",
    "            self.centers = self.data[rand_index, :]\n",
    "            return self.centers\n",
    "        else:\n",
    "            print(\"Method needs to be either 0 or 1\")\n",
    "        ### end of your code\n",
    "        \n",
    "    def search(self):\n",
    "        \"\"\"\n",
    "        update the partitions and the next centers;\n",
    "        here we use centroids for k-means method\n",
    "        \"\"\"\n",
    "        self.partitions={i:[] for i in range(self.k)}\n",
    "        self.next_centers=numpy.array([])\n",
    "        \n",
    "        ### your code starts here\n",
    "        # update the partitions\n",
    "        for sample in range(self.n):  # loop through all the samples\n",
    "            closest_sample = []  # initialize closest samples as a list\n",
    "            for center in range(len(self.centers)):  # loop through all picked centers\n",
    "                euc_dist = numpy.linalg.norm(self.data[sample] - self.centers[center])\n",
    "                closest_sample.append(euc_dist) \n",
    "            closest_index = numpy.argmin(closest_sample)  # return the index of closest sample \n",
    "            self.partitions[sample] = closest_index\n",
    "            \n",
    "        # update the next centers\n",
    "        for i in range(len(self.centers)):  # loop through the number of clusters\n",
    "            partitions_index = self.partitions[i]\n",
    "            selected_centers = self.data[partitions_index]\n",
    "            mean_dist = np.mean(selected_centers)\n",
    "            self.next_centers.append(mean_dist)\n",
    "        self.centers = self.next_centers  # update new center\n",
    "        ### end of your code\n",
    "        \n",
    "    def is_updated(self):\n",
    "        \"\"\"\n",
    "        return True if update is done, but has not yet converged; False otherwise;\n",
    "        the convergence criterion is the sum of absolute relative differences (between self.centers and \n",
    "        self.next_centers) smaller than tol\n",
    "        \"\"\"\n",
    "        \n",
    "        ### your code starts here\n",
    "        converge_criterion = sum(abs(self.centers - self.next_centers))\n",
    "        if not self.update_flag:\n",
    "            print(\"Update_flag needs to be defaulted as True\")\n",
    "        if converge_criterion > self.tol:\n",
    "            return self.update_flag\n",
    "        else:\n",
    "            self.update_flag = False\n",
    "            return self.update_flag\n",
    "        ### end of your code\n",
    "        \n",
    "    def fit_model(self):\n",
    "        \"\"\"\n",
    "        function to fit the k-means algorithms using the above functions\n",
    "        \"\"\"\n",
    "        self.initialize_centers(1)\n",
    "        ### your code starts here\n",
    "        for it in range(self.max_iter):  # perform iterations \n",
    "            self.search()\n",
    "            if not self.is_updated():\n",
    "                print(\"The K-means algorithm is converged!\")\n",
    "                break\n",
    "        ### your code ends here\n",
    "        self.get_labels()\n",
    "\n",
    "    def set_k(self,k):\n",
    "        self.k=k\n",
    "        \n",
    "    def predict(self):\n",
    "        distances = [ numpy.linalg.norm( pt-c ) for c in self.centroids ]\n",
    "        cluster_label = distances.index( min(distances) )\n",
    "        return cluster_label\n",
    "\n",
    "    def get_labels(self):\n",
    "        ### your code starts here\n",
    "        for center in range(len(self.centers)):  # loop through all picked centers\n",
    "            for closest_index in self.partitions[center]:\n",
    "                self.labels[closest_index] = center\n",
    "        self.labels = np.array(self.labels)\n",
    "        ### end of your code\n",
    "        return self.labels\n",
    "    \n",
    "    def get_centers(self):\n",
    "        return self.centers\n",
    "    \n",
    "    def get_clusters(self):\n",
    "        return self.partitions\n",
    "\n",
    "    def get_cost(self):\n",
    "        \"\"\"\n",
    "        Here we use within cluster sum of squares as cost \n",
    "        \"\"\"\n",
    "        ### your code starts here\n",
    "        for sample in range(self.n):  # loop through all the samples\n",
    "            labels = self.labels[sample]\n",
    "            iter_cost = numpy.linalg.norm(self.data[i,:] - self.centers[labels, :])\n",
    "            self.cost += iter_cost\n",
    "        ### end of your code\n",
    "        return self.cost\n",
    "        \n",
    "    def plot_clusters(self):\n",
    "        if self.d>2:\n",
    "            print(\"Dimension too large!\")\n",
    "            return \n",
    "        if self.labels==[]:\n",
    "            self.fit_model()\n",
    "        plt.scatter( self.data[:,0] , self.data[:,1], c=self.labels ,s=3)\n",
    "        plt.scatter( np.array(self.centers)[:,0],np.array(self.centers)[:,1] ,marker='*',c=list(range(self.k)) ,s=300 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Implementing criteria to evaluate clustering algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class clustering_eval_metrics:\n",
    "    def __init__(self, labels: list ,true_labels=None): # label must be between 0 to number_of_labels - 1\n",
    "        self.labels=numpy.array(labels)\n",
    "        self.true_labels=true_labels\n",
    "        self.cmat=None\n",
    "        self.ars=None\n",
    "        \n",
    "    def set_true_labels(self, true_labels):\n",
    "        self.true_labels=numpy.array(true_labels)\n",
    "        \n",
    "    def contingency_matrix(self): \n",
    "        \"\"\"\n",
    "        return a contingency matrix\n",
    "        \"\"\"\n",
    "        ### your code starts here\n",
    "        # here I referred sklearn.metrics.cluster.contingency_matrix to build my own contingency matrix\n",
    "        pred_clusters, pred_cluster_index = np.unique(self.labels, return_inverse=True)  # return the indices of ar\n",
    "        true_clusters, true_clusters_index = np.unique(self.true_labels, return_inverse=True)  # return the indices of ar\n",
    "        num_pred_clusters = pred_clusters.shape[0]  # get dimension sample size \n",
    "        num_true_clusters = true_clusters.shape[0]  # get dimension sample size\n",
    "        \n",
    "        # initialize the contingency matrix\n",
    "        if num_pred_clusters >= num_true_clusters:\n",
    "            n_clusters = num_pred_clusters\n",
    "        else:\n",
    "            n_clusters = num_true_clusters\n",
    "        self.cmat = np.empty((n_clusters, n_clusters))\n",
    "        \n",
    "        # fill-in the contingency matrix\n",
    "        for pos in range(len(true_clusters_index)):\n",
    "            self.cmat[true_clusters_index[pos], pred_cluster_index[pos]] += 1\n",
    "        ### end of your code\n",
    "        \n",
    "        return self.cmat\n",
    "        \n",
    "    def adjusted_rand_score(self):\n",
    "        \"\"\"\n",
    "        return ARI/ARS\n",
    "        \"\"\"\n",
    "        ### your code starts here\n",
    "        # firstly, check input dimensions \n",
    "        if self.labels.ndim != 1:\n",
    "            raise ValueError(\n",
    "                f\"labels_pred must be 1D: shape is: {labels_pred.shape}\")\n",
    "        if self.true_labels.ndim != 1:\n",
    "            raise ValueError(\n",
    "                f\"labels_true must be 1D: shape is: {labels_true.shape}\")\n",
    "        \n",
    "        # get labels dimentionalities, similar steps as building contingency_matrix\n",
    "        num_samples = self.true_labels.shape[0]\n",
    "        pred_clusters = np.unique(self.labels)\n",
    "        num_pred_clusters = pred_clusters.shape[0]\n",
    "        true_clusters = np.unique(self.true_labels)\n",
    "        num_true_clusters = true_clusters.shape[0]\n",
    "        \n",
    "        # Check special limit cases: no clustering or\n",
    "        # trivial clustering where each document is assigned a unique cluster,\n",
    "        # which refers perfect matching case hence return 1\n",
    "        if (num_true_clusters == num_pred_clusters == 1 or num_true_clusters == num_pred_clusters == 0 \\\n",
    "            or num_true_clusters == num_pred_clusters == num_samples):\n",
    "            return 1\n",
    "        else:  # \n",
    "            if self.cmat == None:\n",
    "                self.cmat = self.contingency_matrix()\n",
    "            \n",
    "            # calculate ARI using the contingency matrix \n",
    "            # using the equation of adjusted Rand index \n",
    "            # make a sub-helper function\n",
    "            def _helper_comb(n):\n",
    "                return n*(n-1)/2\n",
    "            \n",
    "            # Here I referred sklearn.metrics.adjusted_rand_score to for ARI calculation\n",
    "            sum_comb_1 = sum(_helper_comb(n1) for n1 in np.ravel(self.cmat.sum(axis=1)))\n",
    "            sum_comb_2 = sum(_helper_comb(n2) for n2 in np.ravel(self.cmat.sum(axis=0)))\n",
    "            comb_mean = (sum_comb_1 + sum_comb_2) / 2\n",
    "            comb_prod = (sum_comb_1 * sum_comb_2) / _helper_comb(num_samples)\n",
    "            \n",
    "            # last component: calculate summation of combinations by loops \n",
    "            sum_comb = 0  # initialize summation of combinations\n",
    "            for i in range(len(self.cmat)):\n",
    "                for j in range(len(self.cmat[0])):\n",
    "                    component = self.cmat[i][j]\n",
    "                    sum_comb += _helper_comb(component)\n",
    "                    \n",
    "        self.ars = (sum_comb - prod_comb) / (mean_comb - prod_comb)\n",
    "        ### end of your code\n",
    "        return self.ars\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. k-medoid algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a class called pam to implement the k-medoid algorithm. It should have a similar structure as the k_means class as we implemented before. Write the code as concise as possible. Any code that exceeds 40 lines will get penalized.\n",
    "\n",
    "pam should take one more parameter p. the input will look like\n",
    "\n",
    "(data: numpy.ndarray, d: int, k: int , tol: float, max_iter: int, p: float)\n",
    "\n",
    "p indicates whtat Lp norm is used. $ |x|_{L_p}=( x_1^p+\\ldots+x_d^p  )^{1/p}  $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Your code starts here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4. Simulation Study \n",
    "\n",
    "### You may choose not to use the functions written above to finish this part. Then, you automatically lose all the points from Part 1~3.\n",
    "\n",
    "Sample $60$ data points each from the following distributions each\n",
    "\n",
    "$$ X_1\\sim N\\bigg(\\begin{pmatrix}\n",
    "0\\\\\n",
    "0\\end{pmatrix},\\begin{pmatrix}\n",
    "1 & 0\\\\\n",
    "0 & 1 \\end{pmatrix}\\bigg),X_2\\sim N\\bigg(\\begin{pmatrix}\n",
    "3\\\\\n",
    "2\\end{pmatrix},\\begin{pmatrix}\n",
    "2 & 1\\\\\n",
    "1 & 1 \\end{pmatrix}\\bigg)\n",
    ", X_3\\sim N\\bigg(\\begin{pmatrix}\n",
    "5\\\\\n",
    "0\\end{pmatrix},\\begin{pmatrix}\n",
    "2 & 1\\\\\n",
    "1 & 1 \\end{pmatrix}\\bigg) $$\n",
    "\n",
    "to form a sample of size $180$.  Use numpy.random.multivariate_normal() and set numpy.random.seed(20) in front."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=numpy.array([])\n",
    "true_label=numpy.array([])\n",
    "\n",
    "### your code starts here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### your code ends here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Apply k-means method (set k=3) to the simulated data set. Plot different clusters and their centers. Also calculate the adjusted rand score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2a Apply pam method (set k=3) to the simulated data set. Plot different clusters and their centers using the L_p \"norm\" when p=.1 and p=2. Also calculate the adjusted rand score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2b Can you compare these results and analyze quantitatively the cause of the difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 How to choose k? First interpret the plot that you get from the code below, then come up with a procedure using this plot to find a k. What's k you would like to use? Explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss=[]\n",
    "km=k_means(data, 2, 1,1e-7,500)\n",
    "for i in range(1,16):\n",
    "    km.k=i\n",
    "    km.fit_model()\n",
    "    wcss.append(km.get_cost())\n",
    "plt.plot(list(range(1,16)),wcss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5. Segment Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the dataset\n",
    "\n",
    "\t\t\n",
    "<h4 align = \"center\">\n",
    "Environment Canada    \n",
    "Monthly Values for July - 2015\t\n",
    "</h4>\n",
    "<html>\n",
    "<head>\n",
    "<style>\n",
    "table {\n",
    "    font-family: arial, sans-serif;\n",
    "    border-collapse: collapse;\n",
    "    width: 100%;\n",
    "}\n",
    "\n",
    "td, th {\n",
    "    border: 1px solid #dddddd;\n",
    "    text-align: left;\n",
    "    padding: 8px;\n",
    "}\n",
    "\n",
    "tr:nth-child(even) {\n",
    "    background-color: #dddddd;\n",
    "}\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Name in the table</th>\n",
    "    <th>Meaning</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><font color = \"green\"><strong>Stn_Name</font></td>\n",
    "    <td><font color = \"green\"><strong>Station Name</font</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><font color = \"green\"><strong>Lat</font></td>\n",
    "    <td><font color = \"green\"><strong>Latitude (North+, degrees)</font></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><font color = \"green\"><strong>Long</font></td>\n",
    "    <td><font color = \"green\"><strong>Longitude (West - , degrees)</font></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Prov</td>\n",
    "    <td>Province</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Tm</td>\n",
    "    <td>Mean Temperature (°C)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>DwTm</td>\n",
    "    <td>Days without Valid Mean Temperature</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>D</td>\n",
    "    <td>Mean Temperature difference from Normal (1981-2010) (°C)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><font color = \"black\">Tx</font></td>\n",
    "    <td><font color = \"black\">Highest Monthly Maximum Temperature (°C)</font></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>DwTx</td>\n",
    "    <td>Days without Valid Maximum Temperature</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><font color = \"black\">Tn</font></td>\n",
    "    <td><font color = \"black\">Lowest Monthly Minimum Temperature (°C)</font></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>DwTn</td>\n",
    "    <td>Days without Valid Minimum Temperature</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>S</td>\n",
    "    <td>Snowfall (cm)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>DwS</td>\n",
    "    <td>Days without Valid Snowfall</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>S%N</td>\n",
    "    <td>Percent of Normal (1981-2010) Snowfall</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><font color = \"green\"><strong>P</font></td>\n",
    "    <td><font color = \"green\"><strong>Total Precipitation (mm)</font></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>DwP</td>\n",
    "    <td>Days without Valid Precipitation</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>P%N</td>\n",
    "    <td>Percent of Normal (1981-2010) Precipitation</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>S_G</td>\n",
    "    <td>Snow on the ground at the end of the month (cm)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Pd</td>\n",
    "    <td>Number of days with Precipitation 1.0 mm or more</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>BS</td>\n",
    "    <td>Bright Sunshine (hours)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>DwBS</td>\n",
    "    <td>Days without Valid Bright Sunshine</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>BS%</td>\n",
    "    <td>Percent of Normal (1981-2010) Bright Sunshine</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>HDD</td>\n",
    "    <td>Degree Days below 18 °C</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>CDD</td>\n",
    "    <td>Degree Days above 18 °C</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Stn_No</td>\n",
    "    <td>Climate station identifier (first 3 digits indicate   drainage basin, last 4 characters are for sorting alphabetically).</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>NA</td>\n",
    "    <td>Not Available</td>\n",
    "  </tr>\n",
    "\n",
    "\n",
    "</table>\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stn_Name</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "      <th>Prov</th>\n",
       "      <th>Tm</th>\n",
       "      <th>DwTm</th>\n",
       "      <th>D</th>\n",
       "      <th>Tx</th>\n",
       "      <th>DwTx</th>\n",
       "      <th>Tn</th>\n",
       "      <th>...</th>\n",
       "      <th>DwP</th>\n",
       "      <th>P%N</th>\n",
       "      <th>S_G</th>\n",
       "      <th>Pd</th>\n",
       "      <th>BS</th>\n",
       "      <th>DwBS</th>\n",
       "      <th>BS%</th>\n",
       "      <th>HDD</th>\n",
       "      <th>CDD</th>\n",
       "      <th>Stn_No</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMAINUS</td>\n",
       "      <td>48.935</td>\n",
       "      <td>-123.742</td>\n",
       "      <td>BC</td>\n",
       "      <td>8.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>273.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1011500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COWICHAN LAKE FORESTRY</td>\n",
       "      <td>48.824</td>\n",
       "      <td>-124.133</td>\n",
       "      <td>BC</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>307.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1012040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LAKE COWICHAN</td>\n",
       "      <td>48.829</td>\n",
       "      <td>-124.052</td>\n",
       "      <td>BC</td>\n",
       "      <td>6.8</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>168.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1012055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DUNCAN KELVIN CREEK</td>\n",
       "      <td>48.735</td>\n",
       "      <td>-123.728</td>\n",
       "      <td>BC</td>\n",
       "      <td>7.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>14.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>267.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1012573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ESQUIMALT HARBOUR</td>\n",
       "      <td>48.432</td>\n",
       "      <td>-123.439</td>\n",
       "      <td>BC</td>\n",
       "      <td>8.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>258.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1012710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Stn_Name     Lat     Long Prov   Tm  DwTm    D    Tx  DwTx  \\\n",
       "0               CHEMAINUS  48.935 -123.742   BC  8.2   0.0  NaN  13.5   0.0   \n",
       "1  COWICHAN LAKE FORESTRY  48.824 -124.133   BC  7.0   0.0  3.0  15.0   0.0   \n",
       "2           LAKE COWICHAN  48.829 -124.052   BC  6.8  13.0  2.8  16.0   9.0   \n",
       "3     DUNCAN KELVIN CREEK  48.735 -123.728   BC  7.7   2.0  3.4  14.5   2.0   \n",
       "4       ESQUIMALT HARBOUR  48.432 -123.439   BC  8.8   0.0  NaN  13.1   0.0   \n",
       "\n",
       "    Tn  ...  DwP    P%N  S_G    Pd  BS  DwBS  BS%    HDD  CDD   Stn_No  \n",
       "0  1.0  ...  0.0    NaN  0.0  12.0 NaN   NaN  NaN  273.3  0.0  1011500  \n",
       "1 -3.0  ...  0.0  104.0  0.0  12.0 NaN   NaN  NaN  307.0  0.0  1012040  \n",
       "2 -2.5  ...  9.0    NaN  NaN  11.0 NaN   NaN  NaN  168.1  0.0  1012055  \n",
       "3 -1.0  ...  2.0    NaN  NaN  11.0 NaN   NaN  NaN  267.7  0.0  1012573  \n",
       "4  1.9  ...  8.0    NaN  NaN  12.0 NaN   NaN  NaN  258.6  0.0  1012710  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename='weather.csv'\n",
    "df = pd.read_csv(filename)\n",
    "df = df[pd.notnull(df[\"Tm\"])]\n",
    "df = df.reset_index(drop=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Basemap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-93250c921ad1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Long'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mllon\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Long'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mulon\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Lat'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mllat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Lat'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mulat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m my_map = Basemap(projection='merc',\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mresolution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'l'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marea_thresh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mllcrnrlon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mllon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllcrnrlat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mllat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Basemap' is not defined"
     ]
    }
   ],
   "source": [
    "rcParams['figure.figsize'] = (14,10)\n",
    "llon=-140\n",
    "ulon=-50\n",
    "llat=40\n",
    "ulat=65\n",
    "df = df[(df['Long'] > llon) & (df['Long'] < ulon) & (df['Lat'] > llat) &(df['Lat'] < ulat)]\n",
    "\n",
    "my_map = Basemap(projection='merc',\n",
    "            resolution = 'l', area_thresh = 1000.0,\n",
    "            llcrnrlon=llon, llcrnrlat=llat,\n",
    "            urcrnrlon=ulon, urcrnrlat=ulat) \n",
    "my_map.drawcoastlines()\n",
    "my_map.drawcountries()\n",
    "my_map.shadedrelief()\n",
    "\n",
    "\n",
    "## this is to change longitude and latitude to coordinates\n",
    "\n",
    "xs,ys = my_map(np.asarray(df.Long), np.asarray(df.Lat))\n",
    "df['xm']= xs.tolist()\n",
    "df['ym'] =ys.tolist()\n",
    "\n",
    "# plot the stations on the map\n",
    "for index,row in df.iterrows():\n",
    "    my_map.plot(row.xm, row.ym,markerfacecolor =([1,0,0]),  marker='o', markersize= 5, alpha = 0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the following, you'll work on two datasets data1 (segmentation based on location data only) and data2 (segmentation based on location data as well as the temperature data) to perform k means methods with an appropriate k to do clustering and then label the clusters on two separate maps. You need to justify every decisions you make by appropriate plots or reasoning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['xm', 'ym'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-15de6091e66b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## do not change anything in this block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata1\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'xm'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ym'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'xm'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ym'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Tx'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Tm'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Tn'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan_to_num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2932\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2933\u001b[0m             indexer = self.loc._convert_to_indexer(key, axis=1,\n\u001b[0;32m-> 2934\u001b[0;31m                                                    raise_missing=True)\n\u001b[0m\u001b[1;32m   2935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter, raise_missing)\u001b[0m\n\u001b[1;32m   1352\u001b[0m                 kwargs = {'raise_missing': True if is_setter else\n\u001b[1;32m   1353\u001b[0m                           raise_missing}\n\u001b[0;32m-> 1354\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1355\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         self._validate_read_indexer(keyarr, indexer,\n\u001b[1;32m   1160\u001b[0m                                     \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m                                     raise_missing=raise_missing)\n\u001b[0m\u001b[1;32m   1162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1244\u001b[0m                 raise KeyError(\n\u001b[1;32m   1245\u001b[0m                     u\"None of [{key}] are in the [{axis}]\".format(\n\u001b[0;32m-> 1246\u001b[0;31m                         key=key, axis=self.obj._get_axis_name(axis)))\n\u001b[0m\u001b[1;32m   1247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m             \u001b[0;31m# We (temporarily) allow for some missing keys with .loc, except in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['xm', 'ym'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "## do not change anything in this block\n",
    "data1= df[['xm','ym']].to_numpy()\n",
    "data2 = df[['xm','ym','Tx','Tm','Tn']].to_numpy()\n",
    "\n",
    "data1 = np.nan_to_num(data1)\n",
    "data1 = StandardScaler().fit_transform(data1)\n",
    "data2 = np.nan_to_num(data2)\n",
    "data2 = StandardScaler().fit_transform(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add your code for problem 3 from part B below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data - see notebook on \"Dimension Reduction, PCA, kernel PCA, Part 1\"\n",
    "\n",
    "# put your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peform hierarchical clustering on the states using complete linkage clustering \n",
    "# (using Euclidean distance) and plot the corresponding denrogram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the states in each cluster and print them\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now standardize the data and perform hierarchical clustering as above\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a \"reasonable\" partition by considering the dedrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Put your answer to Problem 3, part (d) here:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"blue\"> Submit both a pdf file and your original jupyter notebook on canvas.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
